---
created: 2024-06-16 21:50:50
updated: 2024-06-18 00:53:04
draft: false
tags: 
- 机器学习
- 期末
title: 机器学习-期末录音笔记
---

# 机器学习 - 期末录音笔记
选择题（单选）
$3 \times 10=30$
判断题
$2 \times 5 =10$

决策树、Kmeans、集成学习、样本对结果的影响、KNN

都是一个简单的问答题，大部分从PPT可以找到

1. 集成学习(10)
	1. 集成学习的原理
	2. 集成策略有哪些，有哪些对应算法
2. 给定模型的分类划分图（a,b,c）(15)
	1. 分析模型拟合的三种情况
	2. 如何解决以上问题？
3. 给定四个数据点，矩形对角线为同一类(15)
	1. 画一个决策树完成二分类
	2. 决策树减少过拟合的策略有哪些
4. 给定7个数据，特征数为3，特征以及标签均为0,1
	1. 简述朴素贝叶斯分类器进行分类的过程
	2. 给定一个测试数据，用朴素贝叶斯完成分类。





6. 降维的目的
7. 数据降维使用的方式
	- 数据损失、语义持续性
8. KNN（近邻）算法
	- 思路
	- 与其它方法不一样的地方在哪里
		- 无需训练
	- 优点和缺点
9. 分类器之间的差异性
10. k- means
	1. 概念
	2. 特点，在执行过程的敏感点和特殊点，以及对属性的关注
11. **EM 聚类**
	1. E步，M步 的方式的原理
	2. 不要求公式的书写
	3. 理解思路，哪些概率需要关注
12. 混合高斯模型的聚类实现
	1. **推导初始分布、概率、分布估计**
	2. 贝叶斯定理 确定混合高斯的每一个先验概率
13. 线性模型
	1. 线性加权方式，对数据进行判别
		1. **最小二乘法推导**
		2. **Fisher 判别器**
	2. **感知机**
		1. 感知机的构建
		2. **模型训练过程，求解模型参数**
		3. **对偶问题的构建以及优化**
14. SVM
	1. 超平面的定义，如何计算
	2. **目标函数到参数的求解大致思路**
	3. **算法特点（优点缺点）**
	4. **超平面的距离计算，最大间隔，目标函数的计算**
	5. **参数的更新方式**
	6. **KTT 条件（分析最后一个条件）**
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"
    src="https://search.pstatic.net/common?src=https://i.imgur.com/sgcCuQJ.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">title
    </div>
</center>


<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"
    src="https://search.pstatic.net/common?src=https://i.imgur.com/qqis1Ou.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">title
    </div>
</center>

1. 决策树
	1. 相关概念
	2. 如何构建一个决策树
	3. 决策树的剪枝
	4. KLS 方法
	5. 度量方法
	6. 节点的判定
	   • 当前结点中的样本集合为空，即**空叶⼦**；
		• 前结点中的所有样本全部归属于同⼀类别，即**纯叶⼦**；
		• 当前结点中的所有样本在所有属性上取值相同，即**属性被测试完的叶⼦**

2. 神经网络
	1. **掌握公式：权重更新，相似度的计算**
	2. SOM：原理
	3. 下采样，抽样，数据降维
	4. **max-pooling 的细节**
	5. **不同的激活函数的理解和选用**
3. 贝叶斯分类器
	1. 和传统分类器的思路的差异
	2. 后验更新方式
	3. 样本法
	4. **朴素贝叶斯概率计算**
	5. 最大似然估计
4. **朴素贝叶斯**（重点）：
    1. 离散化属性 -->概率
    2. 什么数据有效

---
- 第一章 机器学习概述 13P  

- 第二章 K 近邻算法 4P  
	- 最近邻学习基础知识  
	- **K 近邻算法**  
	- K 近邻模型  
	- **K 近邻算法常见问题**  
	- **降维计算**  
- 第三章 聚类 6P  
	- p3 之后基本都是英文。。。  
	- **K 均值聚类算法**  
	- **K 均值聚类基于 EM 实现**  
	- 基于**高斯混合模型**的聚类实现  
- 第四章 线性系统 5P  
	- 线性模型简介  
	- 感知机模型  
	- 感知机学习策略  
	- 感知机学习算法  
- 第五章 支持向量机 SVM 9P  
	- 这章内容有点多。。  
	- 最大边缘超平面  
	- 线性支持向量机  
	- **非线性支持向量机**  
	- 序列最小最优化算法  
- 第六章 决策树 9P  
	- 决策树学习基本概念  
	- 决策树学习算法  
	- **决策树 CLS 算法**  
	- 决策树算法常见问题  
	- **决策树 C A R T 算法**  
- 第七章 神经网络（一）31p  
	- 神经网络的定义  
	- 神经网络的发展历史  
	- 单层感知机  
	- 神经元模型  
	- **SOM 网络（Self Organizing Maps）**
	- BP 网络  
- 第七章 神经网络（二）12p  
	- SOM 网络  
	- 深度学习的基本概念  
	- 单层感知机与多层感知机  
	- 常见的深度学习网络  
	- 深度学习应用场景  
- 第八章 贝叶斯分类（器）6P  
	- 贝叶斯学习基础  
	- 极大似然估计  
	- **朴素贝叶斯**分类  
	- 贝叶斯网络  
- 第九章 集成学习（boosting）7P  
	- 集成学习概念  
	- **Adaboost** 算法  
	- Bagging  
	- 集成学习结合策略  
	- 集成学习方法


生成模型与判别模型的区别

>[!gemini]+ Gemini
>**生成模型**
> 
> * **目标：**学习联合概率分布 p(x, y)
> * **推断：**通过对边缘概率 p(y) 的积分或求和来计算条件概率 p(y|x)
> * **优点：**
>     * 可以生成新的数据样本
>     * 可以处理缺失数据
> * **缺点：**
>     * 通常计算成本高
>     * 对于高维数据可能不准确
> 
> **判别模型**
> 
> * **目标：**学习条件概率分布 p(y|x)
> * **推断：**直接计算条件概率 p(y|x)
> * **优点：**
>     * 通常计算成本低
>     * 对于高维数据可能更准确
> * **缺点：**
>     * 无法生成新的数据样本
>     * 无法处理缺失数据
 



1.降维的目的
2.数据降维使用的方式
·数据损失、语义持续性

>[!gemini]+ Gemini
>**1. 降维的目的**
> 
> 降维的主要目的是减少数据中特征的数量，同时保留其最重要的信息。这可以通过以下方式实现：
> 
> - **可视化：**减少特征数量使其更容易在低维空间中进行可视化。
> - **计算效率：**更少的特征可以减少计算复杂度和所需存储空间。
> - **提高模型性能：**去除不相关的或冗余的特征可以防止过拟合并提高模型的泛化能力。
> 
> **2. 数据降维的使用的方式**
> 
> **数据损失**
> 
> 降维不可避免地会导致一些数据损失。理想情况下，降维后的数据应该准确地保留原始数据中的相关信息，同时去除冗余和噪声。
> 
> **语义持续性**
> 
> 语义持续性是指降维后数据中保留原始数据的语义含义的能力。例如，降维后的图像应保留其对象识别属性，而降维后的文本应保留其语义内容。
 
k-means（近邻）算法
·思路
·与其它方法不一样的地方在哪里

>[!gemini]+ Gemini
>**k-means 算法**
> 
> **思路：**
> 
> k-means 算法是一种无监督学习算法，用于将数据点聚类到 k 个组中，其中 k 是一个预定义的整数。该算法通过迭代地执行以下步骤来工作：
> 
> 1. **随机初始化 k 个簇中心点：** 这些点通常是数据集中的随机数据点。
> 2. **分配数据点到最近的簇中心点：** 对于每个数据点，将其分配到最接近的簇中心点。
> 3. **更新簇中心点：** 为每个簇重新计算簇中心点，使其等于簇中所有数据点的平均值。
> 4. **重复步骤 2 和 3，直到簇中心点不再变化或达到最大迭代次数：** 在此过程结束时，算法收敛到一组稳定的簇。
> 
> **与其他方法的不同之处：**
> 
> 与其他聚类方法（如层次聚类）相比，k-means 算法具有以下独特之处：
> 
> * **简单易用：** 它实现起来非常简单，并且不需要复杂的优化技术。
> * **高效：** 对于大型数据集，它通常比其他聚类方法更快。
> * **受初始中心点选择的影响：** 算法的性能可能取决于随机初始化的簇中心点。不良的初始化可能会导致局部最优解。
> * **需要预先指定簇数 k：** 这可能具有挑战性，尤其是在簇数未知的情况下。
 
机器学习经典算法中，各种分类器之间的差异性如何？

>[!gemini]+ Gemini
>**线性分类器：**
> 
> * 使用线性决策边界将数据点分类，由权重向量和偏差参数定义。
> * 包括线性回归、逻辑回归。
> 
> **非线性分类器：**
> 
> * 使用非线性决策边界对数据点进行分类。
> * 可以处理更复杂的数据模式。
> * 包括决策树、支持向量机、神经网络。
> 
> **差异性：**
> 
> | 特性 | 线性分类器 | 非线性分类器 |
> |---|---|---|
> | 决策边界 | 线性 | 非线性 |
> | 复杂度 | 相对简单 | 更加复杂 |
> | 可解释性 | 易于解释 | 难以解释 |
> | 泛化能力 | 对噪声和异常值敏感 | 对噪声和异常值更健壮 |
> 
> **具体分类器：**
> 
> **线性回归：**
> 
> * 使用线性方程对连续值进行预测。
> * 权重向量指定每个特征对预测的影响。
> 
> **逻辑回归：**
> 
> * 使用非线性sigmoid函数对二分类问题进行预测。
> * 权重向量指定每个特征对类概率的影响。
> 
> **决策树：**
> 
> * 使用一系列规则将数据点递归地分成更小的子集。
> * 每个节点代表一个特征，每个分支代表该特征的不同值。
> 
> **支持向量机：**
> 
> * 找到将数据点分开的最大间隔超平面。
> * 支持向量是位于间隔边界上的数据点。
> 
> **神经网络：**
> 
> * 受大脑启发的模型，包含神经元层，这些层通过权重相互连接。
> * 可以近似任意函数，用于各种机器学习任务。
 
## KNN

**积极学习**：在训练阶段就对训练样本进行学习处理，构建起分类模型
**消极学习**：没有显式的训练过程 。在训练阶段只是把训练样本保存起来，建模工作延迟到工作阶段才进行处理。最近邻学习就是消极学习

### 算法概述：
- 给定待测样本
- **根据样本之间的距离，找到与目标样本最近的 k 个样本，并根据这 k 个样本的类别来预测目标样本的类别**

**优点：**

- **简单易懂：** KNN 算法的原理非常直观，易于理解和实现。
- **易于实现：** KNN 算法不需要训练模型，只需要存储样本数据，因此实现起来非常简单。
- **无需训练：** KNN 算法不需要像其他机器学习算法那样进行训练，可以节省时间和资源。
- **可用于多种分类问题：** KNN 算法可以用于解决各种分类问题，例如文本分类、图像分类、语音识别等。
- **对噪声数据具有鲁棒性：** KNN 算法对噪声数据有一定的容忍度，不会因为少量噪声数据而导致结果偏差太大。

**缺点：**

- **计算量大：** KNN 算法需要计算目标样本与所有样本之间的距离，当样本数量很大时，计算量会非常大。
- **需要存储所有样本数据：** KNN 算法需要存储所有样本数据，占用内存空间较大。
- **对样本分布敏感：** KNN 算法对样本分布比较敏感，如果样本分布不均匀，可能会导致结果偏差。
- **对噪声数据敏感：** KNN 算法对噪声数据比较敏感，如果噪声数据过多，可能会影响结果的准确性。
- **难以处理高维数据：** KNN 算法在处理高维数据时，计算距离的效率会降低，并且容易出现“维数灾难”问题。

### 算法常用距离

**1. 欧氏距离 (Euclidean Distance)**

$$
d(x, y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}
$$

其中，$x$ 和 $y$ 是两个 $n$ 维向量，$x_i$ 和 $y_i$ 分别表示 $x$ 和 $y$ 的第 $i$ 个元素。欧氏距离是最常用的距离度量之一，它衡量的是两个点之间的直线距离。

**2. 曼哈顿距离 (Manhattan Distance)**

$$
d(x, y) = \\sum_{i=1}^{n}|x_i - y_i|
$$

曼哈顿距离也称为城市街区距离，它衡量的是两个点之间沿着坐标轴方向的距离。

**3. 切比雪夫距离 (Chebyshev Distance)**

$$
d(x, y) = max_{i=1}^{n}|x_i - y_i|
$$

切比雪夫距离衡量的是两个点之间在所有坐标轴上的最大距离。

**4. 明可夫斯基距离 (Minkowski Distance)**

$$
d(x, y) = (\\sum_{i=1}^{n}|x_i - y_i|^p)^{1/p}
$$

明可夫斯基距离是欧氏距离和曼哈顿距离的推广，当 $p=2$ 时，它退化为欧氏距离；当 $p=1$ 时，它退化为曼哈顿距离。

**5. 余弦距离 (Cosine Distance)**

$$
d(x, y) = 1 - \\frac{x \\cdot y}{||x|| ||y||}
$$

余弦距离衡量的是两个向量之间的夹角，它适用于文本分类等应用场景，因为文本向量通常是高维稀疏的。

**6. 汉明距离 (Hamming Distance)**

$$
d(x, y) = \\sum_{i=1}^{n} [x_i \
eq y_i]
$$


*KNN算法中不同K值的选择的影响？*

>[!gemini]+ Gemini
>**KNN算法中不同K值的选择的影响：**
> 
> * **偏差-方差权衡：**较小的K值导致较低的偏差但较高的方差（过拟合），而较大的K值则相反。
> * **噪声敏感性：**较小的K值对噪声数据更敏感，而较大的K值则更具鲁棒性。
> * **计算成本：**较大的K值需要计算更多的距离，从而增加计算成本。
> * **数据分布：**K值的选择应考虑数据的分布。对于离散数据，较小的K值可能更合适，而对于连续数据，较大的K值可能更合适。
> * **经验法则：**通常，K值的最佳值在$[ \sqrt{N}, N ]$之间，其中N是训练数据集中样本的数量。
 
*KNN的分类决策规则，以及它的误分类概率是多少？*

>[!gemini]+ Gemini
>**KNN 的分类决策规则：**
> 
> 将给定查询点分配给训练集中与其距离最小的 $k$ 个点的多数类。
> 
> **误分类概率：**
> 
> 对于一个给定的查询点 $\mathbf{x}$，其被误分类的概率为：
> 
> $$P(\text{error}|\mathbf{x}) = 1 - \max_c P(c|\mathbf{x})$$
> 
> 其中 $c$ 是 $\mathbf{x}$ 可能属于的所有类，$P(c|\mathbf{x})$ 是 $\mathbf{x}$ 属于类 $c$ 的概率，即：
> 
> $$P(c|\mathbf{x}) = \frac{1}{k} \sum_{i=1}^{k} \mathbb{I}(y_i = c)$$
> 
> 其中 $y_i$ 是距离 $\mathbf{x}$ 最近的 $k$ 个点的类别标签，$\mathbb{I}(y_i = c)$ 是指示函数（当 $y_i = c$ 时为 1，否则为 0）。

KNN中如何解决维数灾难？请分点列出常见的KNN数据降维方法。

>[!gemini]+ Gemini
>**解决 KNN 中维数灾难的方法：**
> 
> * **特征选择：**选择最相关的特征，去除冗余和不相关的特征。
> * **特征降维：**将高维数据投影到低维空间，保留重要特征。
> * **基于距离度量优化：**使用非欧几里得距离度量（例如余弦相似度），考虑不同特征的权重。
> * **局部敏感哈希 (LSH)：**一种近似算法，在高维空间中快速找到相似的点。
> * **谱聚类：**将数据点分组到具有相似特征的集群中，然后在每个集群内独立应用 KNN。
> * **随机投影：**将高维数据随机投影到低维空间，同时保留相似性信息。

请给出PCA主成分分析的算法原理，以及计算公式

>[!gemini]+ Gemini
>**主成分分析 (PCA) 算法**
> 
> PCA 算法用于将高维数据投影到低维空间中，同时最大化投影后的方差。优化目标：$min_{W}-tr(\mathbf{W}^{\top}\mathbf{X}\mathbf{X}^{\top}\mathbf{W}), s.t.\mathbf{W}^{\top}\mathbf{W}=1$。算法步骤如下：
> 
> 1. **中心化数据：**将数据中心化，即减去每个特征的平均值。
> 2. **计算协方差矩阵：**计算中心化数据的协方差矩阵$\mathbf{X}\mathbf{X}^{\top}$。
> 3. **计算特征值和特征向量：**对协方差矩阵进行特征分解，得到特征值 $\lambda_1, \lambda_2, ..., \lambda_n$ 和对应的特征向量 $\mathbf{u}_1, \mathbf{u}_2, ..., \mathbf{u}_n$。
> 4. **选择主成分：**根据方差的贡献率选择主成分。一般来说，选取特征值最大的特征向量对应的特征值。
> 5. **构建投影矩阵：**将选定的特征向量组成投影矩阵 $\mathbf{P} = [\mathbf{u}_1, \mathbf{u}_2, ..., \mathbf{u}_k]$，其中 $k$ 是主成分的个数。
> 6. **数据投影：**将中心化数据投影到低维空间中，得到主成分分数：
>    $$\mathbf{Z} = \mathbf{X} \mathbf{P}$$
> 
> 其中 $\mathbf{X}$ 是中心化数据。
> 
> **计算公式**
> 
> 投影后数据的第 $i$ 个主成分分数为：
> $$z_i = \mathbf{x}^T \mathbf{u}_i$$
> 
> 其中 $\mathbf{x}$ 是中心化数据中的一个样本，$\mathbf{u}_i$ 是 $i$ 个主成分对应的特征向量。

## 第三章 聚类

给出K-means算法的输入和输出，以及算法的原理


### 算法原理

>[!gemini]+ Gemini
>**K-means算法**
> 
> **输入：**
> * 数据集 X
> * 簇数 k
> 
> **输出：**
> * 每个数据点的簇分配
> * k 个簇中心
> 
> **算法原理：**
> 
> 1. **随机初始化 k 个簇中心：**在数据集中随机选择 k 个数据点作为初始簇中心。
> 2. **分配数据点到最近的簇中心：**对于每个数据点，将其分配到离它最近的簇中心。
> 3. **重新计算簇中心：**对于每个簇，重新计算簇中心为簇中所有数据点的平均值。
> 4. **重复步骤 2-3：**重复步骤 2 和 3，直到簇中心不再发生明显变化或达到指定的最大迭代次数。
> 
> 该算法的目标是使每个簇中的数据点尽可能接近其簇中心，从而形成紧凑的簇。

K均值聚类基于EM实现的算法原理和具体实现

### EM实现原理

>[!gemini]+ Gemini
>**算法原理**
> 
> K均值聚类是一种基于期望最大化（EM）算法实现的无监督聚类算法。EM算法是一个迭代算法，交替执行两个步骤：
> 
> * **E步（期望）：**计算每个数据点属于每个聚类的概率。
> * **M步（最大化）：**更新聚类中心，使其最大化数据点分配给这些聚类的似然函数。
> 
> **具体实现**
> 
> 对于K均值聚类，EM算法具体实现如下：
> 
> 1. **初始化：**随机初始化K个聚类中心。
> 2. **E步：**计算每个数据点到每个聚类中心的距离，并使用这些距离计算数据点属于每个聚类的概率。
> 3. **M步：**更新每个聚类中心，使其为属于该聚类的所有数据点的平均值。
> 4. **重复2和3步：**直到聚类中心不再变化或达到最大迭代次数。
> 
> **算法步骤**
> 
> 1. **选择K个聚类中心。**
> 2. **反复执行以下步骤：**
>     * **对于每个数据点：**
>         * 计算它到每个聚类中心的距离。
>         * 使用这些距离计算它属于每个聚类的概率。
>     * **对于每个聚类：**
>         * 更新其中心为属于该聚类的所有数据点的平均值。
> 3. **当聚类中心不再变化时，算法停止。**

基于高斯混合模型的聚类实现的数学原理和步骤

### GMM原理实现

>[!gemini]+ Gemini
>**高斯混合模型 (GMM) 聚类**
> 
> **数学原理：**
> 
> GMM 聚类是一种基于概率论的聚类算法。它假设数据点是从一组高斯分布的混合物中生成的。每个高斯分布对应一个聚类，它由均值向量 $\mu_k$ 和协方差矩阵 $\Sigma_k$ 定义。
> 
> GMM 的概率密度函数为：
> 
> $$
> p(x) = \sum_{k=1}^K \pi_k \mathcal{N}(x | \mu_k, \Sigma_k)
> $$
> 
> 其中：
> * $K$ 是聚类数
> * $\pi_k$ 是第 $k$ 个高斯分布的混合系数
> * $\mathcal{N}(x | \mu_k, \Sigma_k)$ 是高斯分布概率密度函数
> 
> **聚类步骤：**
> 
> 1. **初始化：**
>     * 随机初始化聚类中心 $\mu_k$ 和协方差矩阵 $\Sigma_k$
>     * 为每个数据点分配一个初始聚类标签
> 
> 2. **期望最大化 (EM) 算法：**
>     * **E 步：**计算每个数据点属于每个聚类的期望概率：
>         $$
>         p(z_n = k | x_n) = \frac{\pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(x_n | \mu_j, \Sigma_j)}
>         $$
> 
>     * **M 步：**最大化似然函数更新模型参数：
>         * 更新混合系数：
>             $$
>             \pi_k = \frac{1}{N} \sum_{n=1}^N p(z_n = k | x_n)
>             $$
>         * 更新均值向量：
>             $$
>             \mu_k = \frac{\sum_{n=1}^N p(z_n = k | x_n) x_n}{\sum_{n=1}^N p(z_n = k | x_n)}
>             $$
>         * 更新协方差矩阵：
>             $$
>             \Sigma_k = \frac{\sum_{n=1}^N p(z_n = k | x_n) (x_n - \mu_k)(x_n - \mu_k)^T}{\sum_{n=1}^N p(z_n = k | x_n)}
>             $$
> 
> 3. **重复 EM 算法：**重复 E 步和 M 步，直到模型收敛或达到最大迭代次数。
> 
> 4. **聚类分配：**将每个数据点分配给具有最高概率的聚类。

## 第四章 线性模型

什么是线性模型

>[!gemini]+ Gemini
>线性模型是一个函数，其输出值是输入值的线性组合：
> $$
> f(x) = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n
> $$
> 其中 $x_i$ 是输入特征，$w_i$ 是模型的权重系数。



分类和回归的区别是？

>[!gemini]+ Gemini
>**分类与回归的区别**
> 
> **分类**
> 
> * 预测离散值，例如：
>     * 猫 vs. 狗
>     * 0 vs. 1
>     * 红 vs. 蓝
> 
> **回归**
> 
> * 预测连续值，例如：
>     * 房价
>     * 汽车里程数
>     * 人的年龄
> 
> **其他区别**
> 
> * **损失函数：** 分类使用交叉熵损失，而回归使用平方损失或绝对值损失。
> * **评估指标：** 分类使用准确率、召回率、F1-score 等，而回归使用均方根误差 (RMSE)、平均绝对误差 (MAE) 等。
> * **模型：** 分类模型包括逻辑回归、支持向量机、决策树；回归模型包括线性回归、多项式回归、决策树。
 
线性回归模型的最小⼆乘"参数估计" (parameter estimation)求解w和b

>[!gemini]+ Gemini
>采用最小二乘法估计线性回归模型参数 w 和 b 过程如下：
> 
> **目标函数：**
> 
> 定义残差平方和函数为：
> 
> $$J(w, b) = \sum_{i=1}^{n} (y_i - (w x_i + b))^2$$
> 
> 其中，
> 
> * $y_i$ 是第 i 个观测值的真实标签
> * $x_i$ 是第 i 个观测值的特征
> * $w$ 是权重向量
> * $b$ 是偏差项
> 
> **求解过程：**
> 
> 通过求解残差平方和函数的梯度并将其设置为 0，可以获得 w 和 b 的最优值。
> 
> **对 w 求偏导：**
> 
> $$\frac{\partial J}{\partial w} = -2\sum_{i=1}^{n} (y_i - (w x_i + b)) x_i = 0$$
> 
> 求解上述方程得到 w 的最优值：
> 
> $$w = \frac{\sum_{i=1}^{n} (y_i - b) x_i}{\sum_{i=1}^{n} x_i^2}$$
> 
> **对 b 求偏导：**
> 
> $$\frac{\partial J}{\partial b} = -2\sum_{i=1}^{n} (y_i - (w x_i + b)) = 0$$
> 
> 求解上述方程得到 b 的最优值：
> 
> $$b = \frac{1}{n}\sum_{i=1}^{n} (y_i - w x_i)$$
> 
> **最终估计：**
> 
> 将 w 和 b 的最优值代回线性回归方程，即可得到最小二乘参数估计：
> 
> $$y = w x + b$$
 

>[!gemini]+ Gemini
>**线性回归模型的最小二乘参数估计**
> 
> 线性回归模型的最小二乘参数估计的目标是找到一组参数 w 和 b，使得模型对训练数据的预测误差最小。该误差可以用均方误差 (MSE) 来衡量：
> 
> $$ MSE = \frac{1}{n} \sum_{i=1}^n (y_i - (\hat{y}_i))^2 $$
> 
> 其中：
> 
> * n 是训练数据的样本数量
> * y_i 是第 i 个样本的真实标签
> * $\hat{y}_i$ 是模型对第 i 个样本的预测值
> 
> 要最小化 MSE，我们可以对 w 和 b 求导并令导数为 0：
> 
> $$ \frac{\partial MSE}{\partial w} = \frac{2}{n} \sum_{i=1}^n (y_i - (\hat{y}_i)) \cdot (-x_i) = 0 $$
> 
> $$ \frac{\partial MSE}{\partial b} = \frac{2}{n} \sum_{i=1}^n (y_i - (\hat{y}_i)) \cdot (-1) = 0 $$
> 
> 求解这些方程组，可以得到 w 和 b 的最小二乘估计值：
> 
> $$ \hat{w} = \frac{\sum_{i=1}^n x_i y_i - n \bar{x} \bar{y}}{\sum_{i=1}^n x_i^2 - n \bar{x}^2} $$
> 
> $$ \hat{b} = \bar{y} - \hat{w} \bar{x} $$
> 
> 其中：
> 
> * $\bar{x}$ 和 $\bar{y}$ 分别是训练数据 x 和 y 的均值
 


### Fisher 线性判决


Fisher线性判决的特点是什么？区别于⼀般线性判别器的地⽅？

>[!gemini]+ Gemini
>**Fisher线性判别器（FLD）的特点：**
> 
> * **寻找投影方向：**FLD寻找将不同类别的样本投影到一条直线上的方向，使得投影后的样本在该直线上具有最大的类间方差和最小的类内方差。
> * **假设正态分布：**FLD假设不同类别的数据服从正态分布。
> * **线性投影：**FLD只考虑线性投影，将数据投影到一个低维子空间上。
> * **二分类：**FLD适用于二分类问题，即只有两个类别的情况。
> 
> **与一般线性判别器（LDA）的区别：**
> 
> * **假设不同：**LDA可以处理数据服从任意分布的情况，而FLD假设正态分布。
> * **投影方式：**LDA允许投影到任意维度子空间，而FLD只考虑线性投影到一条直线上。
> * **多分类：**LDA可以处理多分类问题，而FLD只能处理二分类问题。
> 
> **其他区别：**
> 
> * FLD更加稳健，对异常值和噪声不敏感。
> * LDA的性能通常优于FLD，尤其当数据不符合正态分布时。
> * FLD的计算成本较低，因为只需要求解一个特征值问题。
 

请尽可能详细的介绍下Fisher线性判决的原理


>[!gemini]+ Gemini
>**Fisher线性判别分析 (FLD)**
> 
> **原理**
> 
> FLD 是一种监督式降维技术，旨在找到一个投影方向，使得不同类别的样本在投影后的空间中的可分性最大化。其基本原理如下：
> 
> 1. **类均值计算：**对于每个类别，计算各个特征的平均值，得到类均值向量。
> 2. **类内散布矩阵和类间散布矩阵：**
>    - 类内散布矩阵：表示每个类别内部样本的方差和协方差，衡量样本在类别内分布的紧密程度。
>    - 类间散布矩阵：表示不同类别均值向量的差异，衡量样本在类别间分布的分散程度。
> 3. **目标函数：**FLD 的目标函数旨在最大化类间散布矩阵和类内散布矩阵的比值，即：
>    $$J(\mathbf{w}) = \frac{|\mathbf{w}^T \mathbf{S}_B \mathbf{w}|}{|\mathbf{w}^T \mathbf{S}_W \mathbf{w}|}$$
>    其中：
>    - $\mathbf{w}$ 是投影方向向量
>    - $\mathbf{S}_B$ 是类间散布矩阵
>    - $\mathbf{S}_W$ 是类内散布矩阵
> 4. **投影方向的求解：**目标函数最大化等价于求解以下广义特征值问题：
>    $$\mathbf{S}_B \mathbf{w} = \lambda \mathbf{S}_W \mathbf{w}$$
>    最大的特征值对应的特征向量即为最佳投影方向。
> 
> **步骤**
> 
> 1. 计算类均值向量。
> 2. 计算类内散布矩阵和类间散布矩阵。
> 3. 求解广义特征值问题，得到投影方向向量。
> 4. 将数据投影到该方向，得到降维后的数据。


#### 最小二乘法推导
线性模型判决器的推导过程中，如何使用最小二乘法进行推导？


>[!gemini]+ Gemini
>**最小二乘法推导线性模型判决器**
> 
> 在给定输入特征 `x` 和输出标签 `y` 的训练数据集下，线性模型判决器可以表示为：
> 
> $$f(x) = w_0 + w_1x_1 + w_2x_2 + \cdots + w_dx_d$$
> 
> 其中 `w` 是模型权重。
> 
> 最小二乘法通过最小化损失函数 `L` 来确定模型权重：
> 
> $$L = \frac{1}{2}\sum_{i=1}^n (y_i - f(x_i))^2$$
> 
> 最小化 `L` 等价于求解以下方程组：
> 
> $$w = (X^TX)^{-1}X^Ty$$
> 
> 其中 `X` 是训练数据集的特征矩阵，`y` 是标签向量，`T` 表示转置。
> 
> **推导过程：**
> 
> 1. 计算梯度：
>    $$\nabla_w L = -X^T(y - Xw)$$
> 
> 2. 令梯度为 0：
>    $$X^T(y - Xw) = 0$$
> 
> 3. 求解 `w`：
>    $$X^TXw = X^Ty$$
>    $$w = (X^TX)^{-1}X^Ty$$
 

### 感知机模型

感知机模型区别于⼀般线性判别器的地⽅？请分点列举

>[!gemini]+ Gemini
>* 感知机模型使用符号符号函数作为激活函数，而一般线性判别器使用线性函数。
> * 感知机模型通过使用错误导向学习算法进行训练，而一般线性判别器通过最小化平方误差进行训练。
> * 感知机模型旨在学习一个超平面，将数据点分为正负类，而一般线性判别器旨在学习一个超平面，将数据点投影到一个新的空间中，以便最大化类间距离。
> * 感知机模型在数据线性可分的情况下表现良好，而一般线性判别器在数据线性不可分的情况下表现更好。
 



感知机学习算法求解的最优化问题，优化过程，以及最终的结果，以及对应的对偶形式

>[!gemini]+ Gemini
>**优化问题**
> 
> 感知机学习算法求解的优化问题为：
> 
> $$\min_\mathbf{w,b} \max_{(\mathbf{x}_1, y_1), \dots, (\mathbf{x}_m, y_m)} \sum_{i=1}^m \max(0, 1-y_i(\mathbf{w}\cdot \mathbf{x}_i + b))$$
> 
> 其中：
> 
> * $\mathbf{w}$ 和 $b$ 是感知机的权重和偏置
> * $\mathbf{x}_i$ 是第 $i$ 个训练样本的特征向量
> * $y_i$ 是第 $i$ 个训练样本的标签（+1 或 -1）
> 
> **优化过程**
> 
> 感知机使用一种贪心的策略来优化上述问题：
> 
> 1. 初始化 $\mathbf{w}$ 和 $b$ 为 0
> 2. 对于每个训练样本 $(\mathbf{x}_i, y_i)$：
>     * 如果 $y_i(\mathbf{w}\cdot \mathbf{x}_i + b) \ge 1$（即样本被正确分类），则继续下一步
>     * 否则，更新 $\mathbf{w} \leftarrow \mathbf{w} + \alpha y_i \mathbf{x}_i$ 和 $b \leftarrow b + \alpha y_i$
> 	    * 其中 $\alpha$ 为学习率
> 
> **最终结果**
> 
> 感知机算法最终收敛于一个解，该解使得：
> 
> $$y_i(\mathbf{w}\cdot \mathbf{x}_i + b) \ge 1, \quad \forall i=1, \dots, m$$
> 
> 也就是说，感知机在训练集上实现了线性可分。
> 
> **对偶形式**
> 
> 感知机优化问题的对偶形式为：
> 
> $$\max_\boldsymbol{\alpha} \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j \mathbf{x}_i \cdot \mathbf{x}_j$$
> 
> 受约束于：
> 
> * $\alpha_i \ge 0, \quad \forall i=1, \dots, m$
> * $\sum_{i=1}^m \alpha_i y_i = 0$
> 
> 其中 $\boldsymbol{\alpha} = (\alpha_1, \dots, \alpha_m)$ 是拉格朗日乘子。


感知机学习算法的对偶形式的基本思想，以及具体的算法过程

>[!gemini]+ Gemini
>**对偶形式的基本思想**
> 
> * 将感知机问题转化为其对偶形式，这将它转换成一个二次规划问题，可以更容易求解。
> * 这样做的好处是，转换后的问题具有稀疏性，这对于大规模数据集非常有益。
> 
> **具体算法过程**
> 
> **输入：** 训练数据 {(x₁, y₁), (x₂, y₂), ..., (xₘ, yₘ)}，其中 xᵢ ∈ Rⁿ，yᵢ ∈ {-1, 1}
> 
> **目标：** 找到一个分离超平面 wᵀx + b = 0，它将正点和负点正确分类。
> 
> **对偶形式：**
> 
> 最大化 $$P(\alpha) = \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j x_i^T x_j$$
> 
> 约束： $$0 \le \alpha_i \le C, \quad i = 1, 2, ..., m$$
> 
> $$\sum_{i=1}^m \alpha_i y_i = 0$$
> 
> 其中αᵢ是拉格朗日乘子，C是惩罚因子。
> 
> **算法步骤：**
> 
> 1. 求解对偶问题的最优解α*。
> 2. 计算决策函数： $$f(x) = sign(w * x + b)$$
>    - w = ∑ᵢαᵢ*yᵢxᵢ
>    - b = yᵢ - wᵀxᵢ，对于任意支持向量 (xᵢ, yᵢ)
> 3. 根据决策函数对新数据进行分类。

## 第五章 SVM

> 在SVM中，什么是超平面和最大边缘超平面?

>[!gemini]+ Gemini
>**超平面**
> 
> 超平面是一个n维空间中的(n-1)维子空间，它将空间划分为两部分。在机器学习中，超平面通常用于将数据点分类到不同的类中。
> 
> **最大边缘超平面**
> 
> 最大边缘超平面是一个将数据点分类为不同类的超平面，其目标是最大化超平面与最近的数据点之间的距离（即边缘）。最大边缘超平面通常用于支持向量机（SVM）中，其中它可以帮助找到一个决策边界，以尽可能准确地分离不同的类。
 
具有**较大边缘**的决策边界相比边缘小的具有更好的**泛化误差**

根据**结构风险最小化原理**，需要设计**最大化**决策边界的**边缘**的线性分类器，以确保最坏情况下的泛化误差最小

SVM中，支持向量如何选取？请给出数学公式定义

>[!gemini]+ Gemini
>$$x_i \in \arg \min_{x_i \in D} \max_{w,b} w^Tx_i+b$$
> 
> 其中：
> 
> * $x_i$ 是数据集中的一条数据
> * $D$ 是训练数据集
> * $w$ 是支持向量机的权重向量
> * $b$ 是支持向量机的偏差
 
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"
    src="https://search.pstatic.net/common?src=https://i.imgur.com/sWwivJw.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">title
    </div>
</center>



拉格朗日乘子法求解线性支持向量机的数学过程，请给出详细的逐步解答



>[!gemini]+ Gemini
>**针对线性支持向量机的拉格朗日对偶问题**
> 
> 设训练集为 $\lbrace (x_i, y_i) \rbrace_{i=1}^{m}$，其中 $x_i \in \mathbb{R}^n$ 为示例，$y_i \in \lbrace -1, 1 \rbrace$ 为标签。
> 
> 线性支持向量机通过求解以下优化问题得到：
> $$\begin{split} \underset{w, b}{\min} &\frac{1}{2}||w||^2 + C \sum_{i=1}^{m} \max(0, 1 - y_i (w^T x_i + b)) \\\ s.t. & y_i(w^T x_i + b) \geq 1 \qquad (\forall i). \end{split}$$
> 
> 其中，$C$ 为正则化参数。
> 
> **推导拉格朗日对偶问题：**
> 
> 我们引入拉格朗日乘子 $\alpha_i \geq 0 (\forall i)$。
> 
> 拉格朗日函数为：
> $$L(w, b, \alpha) = \frac{1}{2}||w||^2 + C \sum_{i=1}^{m} \max(0, 1 - y_i (w^T x_i + b)) - \sum_{i=1}^{m} \alpha_i (y_i (w^T x_i + b) - 1).$$
> 
> **对偶问题的求解：**
> 
> 通过对 $w$ 和 $b$ 求偏导并令其为 $0$，得到：
> $$\begin{split} w &= \sum_{i=1}^{m} \alpha_i y_i x_i \\\ \sum_{i=1}^{m} \alpha_i y_i & = 0. \end{split}$$
> 
> 将这些值代回 $L$，得到拉格朗日对偶函数：
> $$g(\alpha) = \sum_{i=1}^{m} \alpha_i - \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_i \alpha_j y_i y_j x_i^T x_j.$$
> 
> 拉格朗日对偶问题为：
> $$\underset{\alpha}{\max} \ g(\alpha) \\\ \text{s.t.} \ \alpha_i \geq 0 \qquad (\forall i) \\\ \sum_{i=1}^{m} \alpha_i y_i = 0.$$


### 基于核函数的SVM


基于核函数的SVM具体是如何实现的，请给出每一步的数学过程

>[!gemini]+ Gemini
>**支持向量机 (SVM) 与核函数**
> 
> SVM 是监督学习算法，用于分类或回归任务。其基本思想是在数据点之间找到一个最大间隔超平面，将数据点分开。
> 
> **核函数**允许 SVM 处理非线性可分数据。核函数将原始输入空间映射到更高维的特征空间，在该空间中数据变得线性可分。
> 
> **基于核函数的 SVM**
> 
> 假设我们有一组数据点：
> 
> $$(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$$
> 
> 其中 $x_i$ 是输入向量，$y_i$ 是目标标签（+1 或 -1）。
> 
> **步骤 1：特征映射**
> 
> 使用核函数 $\phi(\cdot)$ 将输入空间 $X$ 映射到特征空间 $F$：
> 
> $$x \mapsto \phi(x)$$
> 
> **步骤 2：最大间隔超平面的求解**
> 
> 在特征空间 $F$ 中求解最大间隔超平面：
> 
> $$\mathbf{w}^T \phi(x) + b = 0$$
> 
> 其中 $\mathbf{w}$ 是超平面的权重向量，$b$ 是偏置。
> 
> **步骤 3：拉格朗日乘数**
> 
> 使用拉格朗日乘数将约束最大化问题转换为无约束优化问题：
> 
> $$L(\mathbf{w}, b, \alpha) = \frac{1}{2}\mathbf{w}^T\mathbf{w} - \sum_{i=1}^n\alpha_i[y_i(\mathbf{w}^T\phi(x_i) + b) - 1]$$
> 
> 其中 $\alpha_i$ 是拉格朗日乘数。
> 
> **步骤 4：对偶问题**
> 
> 将拉格朗日函数对 $\mathbf{w}$ 和 $b$ 求极值，得到对偶问题：
> 
> $$\min_{\alpha} \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\phi(x_i)^T\phi(x_j) - \sum_{i=1}^n\alpha_i$$
> 
> s.t.
> $$0 \le \alpha_i \le C$$
> $$\sum_{i=1}^n\alpha_iy_i = 0$$
> 
> 其中 $C$ 是正则化参数。
> 
> **步骤 5：使用核技巧**
> 
> 在特征空间中计算内核矩阵：
> 
> $$K_{ij} = \phi(x_i)^T\phi(x_j)$$
> 
> 然后，将内核技巧应用于对偶问题：
> 
> $$\min_{\alpha} \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_jK_{ij} - \sum_{i=1}^n\alpha_i$$
> 
> s.t.
> $$0 \le \alpha_i \le C$$
> $$\sum_{i=1}^n\alpha_iy_i = 0$$
> 
> **步骤 6：求解$\alpha_i$**
> 
> 求解对偶问题，得到 $\alpha_i$。基于核函数的 SVM 的决策函数为：
> 
> $$f(x) = \operatorname{sign}\left(\sum_{i=1}^n\alpha_iy_iK_{ix} + b\right)$$

SVR的思路是怎样的

>[!gemini]+ Gemini
>支持向量回归 (SVR) 是一种监督式机器学习算法，用于解决回归问题。其思想类似于支持向量机 (SVM) 算法，通过使用核函数将输入数据映射到高维空间，然后在这个高维空间中寻找一个最大间隔超平面，该超平面与训练样本的距离最大化。
> 
> SVR 算法的具体思路如下：
> 
> 1. **数据映射：**将输入数据通过核函数映射到高维空间，从而增强数据的非线性特征。
> 2. **损失函数：**使用 ε-不敏感损失函数，该函数忽略落在 ε 范围内的误差，只对超出该范围的误差进行惩罚。
> 3. **优化问题：**目标是找到一个最大间隔超平面，使得所有训练样本都落在 ε 范围内的带状区域内，同时使超平面与带状区域的距离最大化。
> 4. **对偶问题：**将优化问题转化为一个对偶问题，该问题更容易求解。
> 5. **支持向量：**找到落在带状区域边界上的训练样本，这些样本称为支持向量。
> 6. **预测：**对于新的输入数据，通过计算其到超平面的距离来进行预测。
> 
> 超平面由以下公式表示：
> 
> $$ f(x) = w^T \phi(x) + b $$
> 
> 其中：
> 
> * $w$ 是权重向量
> * $\phi(x)$ 是核函数映射后的特征向量
> * $b$ 是偏差项
> 
> SVR 算法的优势在于它能够很好地处理非线性回归问题，并且对异常值和噪声具有鲁棒性。
 



## 第五章 决策树


### CLS算法


请你介绍决策树CLS算法的基本思想，以及对应的决策树构建步骤


>[!gemini]+ Gemini
>**决策树 CLS 算法的基本思想**
> 
> 决策树 CLS（类标签平滑）算法是一种决策树算法，它通过对数据进行递归划分来构建决策树，以实现分类任务。其基本思想是：
> 
> * 将数据集划分为更小的同质子集。
> * 为每个子集选择最佳划分属性。
> * 递归地应用上述步骤，直到所有子集都成为纯子集（即所有样本都属于同一类）。
> 
> **决策树构建步骤**
> 
> 决策树的构建步骤如下：
> 
> 1. **选择根节点：**从训练数据集选择一个属性作为根节点。这是根据信息增益或基尼不纯度等指标来选择的。
> 
> 2. **递归构建子树：**对于每个根节点属性值，将数据集划分为子集。递归地为每个子集重复步骤 1-4。
> 
> 3. **选择划分属性：**对于每个子集，从剩余属性中选择最佳划分属性。
> 
> 4. **创建叶节点：**当一个子集不能进一步划分时（例如，所有样本都属于同一类），则将其标记为叶节点。
> 
> 5. **停止划分：**当所有训练数据都已被分配到叶节点时，停止递归。
> 
> **叶节点的标签分配**
> 
> CLS 算法的一个独特之处在于，它使用类标签平滑来分配叶节点的标签。对于每个叶节点，它将分配一个平滑的类标签概率分布，而不是单个类标签。该分布基于训练集中落入叶节点的样本的类标签频率计算。

#### 信息增益的计算

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"
    src="https://search.pstatic.net/common?src=https://i.imgur.com/o4IwYMc.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">title
    </div>
</center>


<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"
    src="https://search.pstatic.net/common?src=https://i.imgur.com/9kgRzku.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">title
    </div>
</center>


<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"
    src="https://search.pstatic.net/common?src=https://i.imgur.com/jNCavU2.png">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">title
    </div>
</center>



## ANN


神经网络中权重更新，相似度的计算公式。以及具体到BP网络的相关公式

>[!gemini]+ Gemini
>**权重更新**
> 
> 在神经网络中，权重是调整神经元输出以最小化损失函数的参数。权重更新规则包括：
> 
> * **梯度下降**：
> 
> $$w_{new} = w_{old} - \alpha \nabla L(w)$$
> 
> 其中：
> * $w$ 是权重向量
> * $\alpha$ 是学习率
> * $\nabla L(w)$ 是损失函数关于权重的梯度
> 
> * **动量**：
> 
> $$v_{new} = \beta v_{old} + (1 - \beta) \nabla L(w)$$
> 
> $$w_{new} = w_{old} - \alpha v_{new}$$
> 
> 其中：
> * $v$ 是动量项
> * $\beta$ 是动量衰减因子
> 
> * **自适应学习率**（如 Adam）：
> 
> $$m_{new} = \beta_1 m_{old} + (1 - \beta_1) \nabla L(w)$$
> 
> $$v_{new} = \beta_2 v_{old} + (1 - \beta_2) (\nabla L(w))^2$$
> 
> $$w_{new} = w_{old} - \alpha \frac{m_{new}}{\sqrt{v_{new} + \epsilon}}$$
> 
> 其中：
> * $m$ 是一阶矩估计
> * $v$ 是二阶矩估计
> * $\beta_1$ 和 $\beta_2$ 是衰减因子
> * $\epsilon$ 是避免除以零的小常数
> 
> **相似度的计算**
> 
> 衡量两个向量或数据点相似度的常用计算方法包括：
> 
> * **余弦相似度**：
> 
> $$sim(x, y) = \frac{x \cdot y}{\|x\| \ \|y\|}$$
> 
> 其中：
> * $x$ 和 $y$ 是向量
> * $\cdot$ 是点积
> * $\|\cdot\|$ 是范数
> 
> * **欧式距离**：
> 
> $$dist(x, y) = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \cdots + (x_n - y_n)^2}$$
> 
> 其中：
> * $x$ 和 $y$ 是向量
> * $n$ 是向量中的元素数量
> 
> * **曼哈顿距离**：
> 
> $$dist(x, y) = |x_1 - y_1| + |x_2 - y_2| + \cdots + |x_n - y_n|$$
> 
> 其中：
> * $x$ 和 $y$ 是向量
> * $n$ 是向量中的元素数量
> 
> **BP 网络相关公式**
> 
> **前向传播**
> 
> * $$a^l = \sigma(z^l)$$
> 
> $$z^l = W^l a^{l-1} + b^l$$
> 
> 其中：
> * $a^l$ 是第 $l$ 层的激活值
> * $z^l$ 是第 $l$ 层的加权和
> * $W^l$ 是第 $l$ 层的权重矩阵
> * $b^l$ 是第 $l$ 层的偏置向量
> * $\sigma$ 是激活函数
> 
> **反向传播**
> 
> * **误差项**
> 
> $$\delta^L = \nabla_a L(a^L) \odot \sigma'(z^L)$$
> 
> $$\delta^l = (W^{l+1})^T \delta^{l+1} \odot \sigma'(z^l), \ \ \ l < L$$
> 
> 其中：
> * $\delta^l$ 是第 $l$ 层的误差项
> * $L$ 是网络的层数
> * $\nabla_a L(a^L)$ 是损失函数关于输出激活值的梯度
> * $\sigma'$ 是激活函数的导数
> 
> * **权重更新**
> 
> $$W^{l+1} = W^{l+1} - \alpha \delta^{l+1} (a^l)^T$$
> 
> $$b^{l+1} = b^{l+1} - \alpha \delta^{l+1}$$
> 
> 其中：
> * $\alpha$ 是学习率

ANN中SOM网络的原理

>[!gemini]+ Gemini
>**SOM 网络的原理**
> 
> 自组织映射 (SOM) 是一种无监督神经网络模型，它将高维输入数据映射到低维（通常为二维）的输出格，也称为自组织映射。
> 
> **工作原理：**
> 
> * **初始化：**SOM 网络的权重向量随机初始化为与输入向量的维数相同的维度。
> * **竞争：**对于给定的输入向量，网络中每个神经元的权重向量与输入向量计算欧几里德距离。距离最小的神经元称为**胜出神经元**。
> * **合作：**胜出神经元及其相邻神经元的权重向量通过竞争学习算法进行更新，使它们更接近输入向量。
> * **自组织：**通过多次迭代此过程，权重向量逐渐自组织成低维映射，其中相似的输入向量被映射到相邻输出节点。
> 
> **目标函数：**
> 
> 通常用于 SOM 训练的目标函数为：
> 
> $$E = \sum_{i=1}^N \min_{j=1}^K \Vert x_i - m_j \Vert^2$$
> 
> 其中：
> 
> * $x_i$ 是输入向量
> * $m_j$ 是 SOM 网络的权重向量
> * $N$ 是输入数据集的大小
> * $K$ 是输出格的大小
> 
> **应用：**
> 
> SOM 网络广泛用于数据可视化、聚类和降维等应用，例如：
> 
> * 可视化高维数据
> * 检测数据集中的模式和异常值
> * 数据压缩和降维
> * 音频和图像处理


ANN中各种常见的激活函数有哪些，它们适合哪些情况下选用？

>[!gemini]+ Gemini
>**常见的 ANN 激活函数：**
> 
> * **Sigmoid 函数：**$$f(x) = \frac{1}{1 + e^{-x}}$$
>   * 适用于二分类问题，输出范围为 (0, 1)。
> 
> * **tanh 函数：**$$f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
>   * 类似于 Sigmoid 函数，但输出范围为 (-1, 1)。
> 
> * **ReLU 函数：**$$f(x) = \max(0, x)$$
>   * 非饱和函数，具有较快的训练速度。
>   * 适用于图像分类、自然语言处理等任务。
> 
> * **Leaky ReLU 函数：**$$f(x) = \max(0.01x, x)$$
>   * ReLU 函数的变体，可解决 ReLU 函数在负值输入时梯度消失的问题。
> 
> * **ELU 函数：**$$f(x) = \begin{cases}x & \text{if } x \geq 0 \\\alpha (e^x - 1) & \text{if } x < 0\end{cases}$$
>   * 与 Leaky ReLU 函数类似，但具有更平滑的负值梯度。
> 
> * **Swish 函数：**$$f(x) = x \cdot \sigma(x)$$
>   * 其中 σ(x) 为 Sigmoid 函数。
>   * 它是 ReLU 和 Sigmoid 函数的组合，具有两者的优点。
> 
> **选择激活函数的考虑因素：**
> 
> * **任务类型：**二分类、回归、多分类等。
> * **输出范围：**激活函数的输出范围应与任务匹配（例如，二分类需要 (0, 1) 范围）。
> * **饱和性：**饱和函数（如 Sigmoid、tanh）在极值时具有接近常数的梯度，可能导致学习速度变慢。
> * **计算效率：**ReLU 和 Leaky ReLU 等非饱和函数在训练时具有更快的收敛速度。
> * **负值输入：**对于具有负值输入的任务，Leaky ReLU 或 ELU 等激活函数可以避免梯度消失问题。

## 贝叶斯分类器


SOM和K-means 的对比，异同点

>[!gemini]+ Gemini
>**异同点**
> 
> **相同点**
> 
> * SOM 和 K-means 都是无监督学习算法，用于将数据点聚类成离散组。
> * 它们都基于欧几里德距离或相似度度量来计算数据点之间的距离。
> * 它们都输出一组聚类中心或代表，代表每个簇的中心。
> 
> **不同点**
> 
> * **数据表示：**SOM 将数据点映射到二维网格中，而 K-means 仅将数据点分配给聚类中心。
> * **自组织：**SOM 是自组织的，这意味着它能够从数据中学习聚类结构，而 K-means 需要预先指定聚类数量。
> * **拓扑保留：**SOM 保留了数据中的拓扑结构，这意味着相似的点在网格上接近，而不同的点在网格上远离。K-means 则没有此特性。
> * **可视化：**SOM 提供了一个二维网格的可视化，这可以帮助理解聚类结构，而 K-means 的可视化通常更难解释。
> * **复杂度：**SOM 通常比 K-means 计算成本更高，因为需要维护二维网格。
> 
> **应用**
> 
> * **SOM：**图像处理、文本挖掘、生物信息学
> * **K-means：**客户细分、市场调研、图像分割
 